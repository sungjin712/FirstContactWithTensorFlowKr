4. SINGLE LAYER NEURAL NETWORK IN TENSORFLOW

In the preface, I commented that one of the usual uses of Deep Learning includes pattern recognition. Given that, in the same way that beginners learn a programming language starting by printing “Hello World” on screen, in Deep Learning we start by recognizing hand-written numbers.

이번 서문에서는, 패턴 인식을 포함한 딥러닝의 대표적인 사용사례에 대해 소개 하겠다.  초보자들이 처음 프로그래밍 언어를 배울때 "Hello, World"를 화면에 출력하는 것과 같이, 딥러닝은 손글씨로 쓰여진 숫자를 인식하는 것으로 시작한다.

In this chapter I present how to build, step by step, a neural network with a single layer in TensorFlow. This neural network will recognize hand-written digits, and it’s based in one of the diverse examples of the beginner’s tutorial of Tensor-Flow[27].

이번 앱터에서는, 텐서플로우 상에서 신경망을 단계별로 빌드하는 방법을 설명하고자 한다. 많은 예제들 중에서, 텐서플로우[27] 사이트에 초보자 사용법에 나와있는 숫자 손글씨 예제를 활용할 것이며, 우리가 구축하는 신경망은 이 손글씨 숫자를 인식하는 용도이다.

Given the introductory style of this book, I chose to guide the reader while simplifying some concepts and theoretical justifications at some steps through the example.

이 책은 소개를 하는 용도로 제작되었기 때문에, 예제에 있는 몇몇 단계별 중에는 개념들과 이론적 증명들을 간단하게 하여 독자들을 가이드 하기로 선택하였다.

If the reader is interested in learn more about the theoretical concepts of this example after reading this chapter, I suggest to read Neural Networks and Deep Learning [28], available online, presenting this example but going in depth with the theoretical concepts.

만약 독자들 중에서, 이번 장을 읽은 후 이론적인 개념들을 배우는 것에 흥미가 있다면, Neural Networks and Deep Learning [28]이란 책을 읽도록 권하겠다. 이 책은 온라인에 공개 되 있으며, 이번 예제에 심화된 이론적 개념들이 추가 된 내용들이 있기 때문이다.

The MNIST data-set

The MNIST data-set is composed by a set of black and white images containing hand-written digits, containing more than 60.000 examples for training a model, and 10.000 for testing it. The MNIST data-set can be found at the MNIST database[29].

MNIST 데이터 셋은 손글씨 숫자들이 흑백 이미지로 구성되 있으며, 학습 모델을 위한 60,000개의 예제들 및 검증을 위한 10,000개 이상의 예제로 구성되어 있다.  MNIST 데이터셋은 MNIST 데이터베이스[29]에서 찾을 수 있다.


This data-set is ideal for most of the people who begin with pattern recognition on real examples without having to spend time on data pre-processing or formatting, two very important steps when dealing with images but expensive in time.

이 데이터 셋은 패턴인식을 시작하는 사람들에게 적합한 데이터 셋이다. 이 데이터 셋은 보통 이미지를 다룰 때, 가장 중요한 2가지 단계지만 시간 소모량이 큰 데이터 전처리와 형식 통일을 할 필요가 없다.


The black and white images (bilevel) have been normalized into 20×20 pixel images, preserving the aspect ratio. For this case, we notice that the images contain gray pixels as a result of the anti-aliasing [30] used in the normalization algorithm (reducing the resolution of all the images to one of the lowest levels). After that, the images are centered in 28×28 pixel frames by computing the mass center and moving it into the center of the frame. The images are like the ones shown here:

이 흑백 이미지 (이중 레벨)는 형상비를 유지하면서 20 × 20 픽셀의 이미지로 정규화 되어있다. 이 경우에는 정규화 알고리즘에 활용되는 엘리어싱 제거 [30] (모든 이미지의 해상도를 최저 이미지의 해상도로 감소)를 적용한 결과로 회색 픽셀들이 포함되 있다.  그 후에 이미지들은 질량 중심을 계산하고 프레임의 중심으로 이동하는 방법을 활용하여 28 × 28 픽셀 프레임들에 중앙화 하였다. 이미지들은 아래와 같은 방식으로 표현 되어있다.

image034

Also, the kind of learning required for this example is supervised learning; the images are labeled with the digit they represent. This is the most common form of Machine Learning.

또한, 이번 예제에서 요구되는 학습방법은 지도학습이다. 각각의 이미지들은 의미하는게 무엇인지 숫자로 표기되있다. 이 학습법은 머신러닝에서 가장 많이 활용되는 형태 중 하나이다.

In this case we first collect a large data set of images of numbers, each labelled with its value. During the training, the model is shown an image and produces an output in the form of a vector of scores, one score for each category. We want the desired category to have the highest score of all categories, but this is unlikely to happen before training.

먼저, 각각 표기가 된 많은 숫자 이미지들의 데이터 셋을 수집한다. 훈련 중에는, 이 모델은 이미지와 결과물을 도출해 낼 것이다. 결과물들은 각각의 점수를 각각의 카테고리별로 나누어진 백터 형태의 점수형으로 되어 있다. 전체 카테고리에서 원하는 카테고리가 가장 높은 점수를 얻기를 기대하지만, 훈련 전에는 발생할 가능성이 높지 않다.

We compute an objective function that measures the error (as we did in previous chapters) between the output scores and the desired pattern of scores. The model then modifies its internal adjustable parameters , called weights, to reduce this error. In a typical Deep Learning system, there may be hundreds of millions of these adjustable weights, and hundreds of millions of labelled examples with which to train the machine. We will consider a smaller example in order to help the understanding of how this type of models work.

우선, 결과 스코어와 기대 패턴 스코어 값들의 오류를 나타내는 목적함수를 계산한다. 그 다음, 오류를 감소 시키기 위해 내부 조정 가능한 파라미터(매개 변수) 값의 가중치(Weight)을 수정한다.  일반적인 딥러닝 시스템에서는 기계에 학습시키기 위해, 아마도 수억개의 조정 가능한 가중치들과 수억개의 레이블이 있는 예제들을 보유하고 있을 것이다.  이해를 돕기 위하여 이러한 종류의 모델들이 어떻게 작동하는지 알기 위해 소규모의 예제들을 사용하였다.

To download easily the data, you can use the script input_data.py [31], obtained from Google’s site [32] but uploaded to the book’s github for your comodity. Simply download the code input_data.py in the same work directory where you are programming the neural network with TensorFlow. From your application you only need to import and use in the following way:

Google 사이트 [32]에서 input_data.py [31] 스크립트를 활용하면 쉽게 데이터를 받을 수 있다. 물론, 편의성을 위해서 책 github에 업로드도 해 놓았다.  신경망을 사용한 텐서플로우로 프로그래밍을 한 작업 디렉토리에 input_data.py를 다운로드 하기만 하면 된다. 그 이후, 어플리케이션에서 아래와 같은 방법으로 import 하기만 하면 된다:

import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

After executing these two instructions you will have the full training data-set in mnist.train and the test data-set in mnist.test. As I previously said, each element is composed by an image, referenced as “xs”, and its corresponding label “ys”, to make easier to express the processing code. Remember that all data-sets, training and testing, contain “xs” and “ys”; also, the training images are referenced in mnist.train.images and the training labels in mnist.train.labels.

위에 나온 2단계의 설명을 따르면, 전체 학습 데이터셋인 mnist.train과 테스트 데이터셋인 mnist.test를 보유 할 수 있다. 전에 언급한대로, 각각의 요소들은 "xs"는 구성되어 있는 이미지, "ys"는 프로세싱 코드의 표현을 좀 더 쉽게 하기 위한 이미지들의 래이블로 표현된다.  모든 데이터셋, 학습 및 검증 과정들은 "xs"나 "ys"를 포함하고 있고, 학습 이미지는 mnist.train.images에 학습 래이블은 mnist.train.labels에 있다.

As previously explained, the images are formed by 28×28 pixels, and can be represented as a numerical matix. For example, one of the images of number 1 can be represented as:
이전에 설명했던 대로, 이미지들은 28*28 픽셀들로 이루어져 있고, 각각 행렬의 수치치값으로 표현 될 수 있다. 예를 들면, 숫자 1을 표현하는 이미지중 하나는 아래와 같이 표현 할 수 있다:

image036

Where each position indicates the level of lackness of each pixel between 0 and 1. This matrix can be represented as an array of 28×28 = 784 numbers. Actually, the image has been transformed in a bunch of points in a vectorial space of 784 dimensions. Only to mention that when we reduce the structure to 2 dimensions, we can be losing part of the information, and for some computer vision algorithms this could affect their result, but for the simplest method used in this tutorial this will not be a problem.

이미지에 표현된 각 위치의 0 과 1 사이의 픽셀 값은, 명암의 정도로 표시되어 있다. 또한, 전체 이미지의 행렬은 28×28 = 784 수의 배열로 표현 되있다. 실질적으로 이 이미지는 784 차원의 백터 공간 값 안에 있는 수 많은 점들로 변환 되 있다.  만약에 이 이미지를 2차원 구조로 축소시킨다면, 일부 관련 정보들을 잃을 수 있다.  몇몇 컴퓨터 영상 알고리즘에서는 결과에도 영향을 미치지만, 이번 튜토리얼은 간단한 접근법이기 때문에 문제가 되지 않는다.

Summarizing, we have a tensor mnist.train.images in 2D in which calling the fuction get_shape() indicates its shape:

요약하자면, TensorShape([Dimension(60000), Dimension(784)])으로 구성된 get_shape()의 함수를 포함 한 2D 텐서인 mnist.train.images를 보유 한 것이다.

The first dimension indexes each image and the second each pixel in each image. Each element of the tensor is the intensity of each pixel between 0 and 1.

첫번째 차원은 이미지 각각에 대한 인덱스이며, 두번째 차원은 이미지 안의 픽셀을 나타낸다. 텐서의 각각의 요소들은 0과 1사이의 화소의 강도 값 (백:0, 흑:1) 으로 이루어져 있다.

Also, we have the labels in the form of numbers between 0 and 9, indicating which digit each image represents. In this example, we are representing labels as a vector of 10 positions, which the corresponding position for the represented number contains a 1 and the rest 0. So mnist.train.labelses is a tensor shaped as TensorShape([Dimension(60000), Dimension10)]).

또한, 각각의 이미지를 어떤 숫자인지 표현하기 위해 0에서 9까지 숫자에 라벨을 표기 하였다. 이번 예제에서는, 숫자의 위치의 포함 여부에 따라 1이나 그 외에는 0으로 표현 된 10가지 위치들로 구성된 백터값을 라벨화 했다. 위의 사항들 때문에, 전체 구성도를 보면, mnist.train.labels은 TensorShape([Dimension(60000), Dimension10)])으로 이루어져 있다.

An artificial neutron
인공 신경망


Although the book doesn’t focus on the theoretical concepts of neural netwoks, a brief and intuitive introduction of how neurons work to learn the training data will help the reader to undertand what is happening. Those readers that already know the theory and just seek how to use TensorFlow can skip this section.
Let’s see a simple but illustrative example of how a neuron learns. Suppose a set of points in a plane labeled as “square” and “circle”. Given a new point “X”, we want to know which label corresponds to it:

이 책은 신경망의 이론적 개념에 집중하고 있지 않지만, 독자들의 이해를 돕기 위하여 간단하고 쉽게 신경망이 어떻게 학습 데이터를 학습하는 과정에 대해서 소개하겠다. 만약 신경망에 대한 이론을 알고 있는 독자들은 바로 Tensorflow 설명 부분으로 넘어가는 것을 추천한다.
신경망이 어떻게 학습하는 것에 대한 간단한 삽화와 함께 확인해보자. 2차원 평면에 "사각형"과 "원"으로 레이블 된 점들이 있다고 가정해보자. 새로운 포인트 "X"가 주어졌을 때, 어떤 레이블이 연관 되었는지 알고 싶다.


Screen Shot 2016-02-16 at 09.30.14

A usual approximation could be to draw a straight line dividing the two groups and use it as a classifier:

일반적인 근사치로 표현하면, 그룹을 2개로 나누는 선을 긋고 이 것을 분류의 기준으로 활용한다.

Screen Shot 2016-02-16 at 09.30.09

In this situation, the input data is represented by vectors shaped as (x,y) representing the coordinates in this 2-dimension space, and our function returning ‘0’ or ‘1’ (above or below the line) to know how to classify it as a “square” or “circle”. Mathematically, as we learned in the linear regression chapter, the “line” (classifier) can be expressed as y= W*x+b.

이 상황에서는 입력 데이터는 2차원 공간의 (x,y)로 표현되는 백터 값으로 표현된다. 또한, "사각형" 인지 "원"인지 분류되는 과정을 알기 위해 '0' 또는 '1' (직선의 윗부분과 아랫부분)으로 나누어진다.  선형회귀 장에서 배운 것처럼, 수학적으로 "선" (분류기)는 y = W*x + b로 표현된다.

Generalizing, a neuron must learn a weight W (with the same dimension as the input data X) and an offset b (called bias in neural networks) to learn how to classify those values. With them, the neuron will compute a weighted sum of the inputs in X using weight W, and add the offset b; and finally the neuron will apply an “activation” non-linear function to produce the result of “0” or “1”.
The function of the neuron can be more formally expressed as:

일반적으로, 신경은 점들의 값을 분류하기 위해서 가중치인 W (입력 데이터 X와 같은 차원)와 오프셋 b (신경망에서는 bias라고 표현)들을 학습시켜야 한다.  이러한 사항들을 기반으로, 신경은 가중치 W를 사용하여 입력 데이터 X의 총 합을 구하고, 오프셋 b를 더한다.  마지막으로 신경은 결과값인 "0"과 "1"을 생성하기 위해 비선형 함수인 "활성화 (Activation)" 함수를 적용한다.  뉴런의 함수를 좀 더 형식적으로 표현하면 다음과 같다.

image043

Having defined this function for our neuron, we want to know how the neuron can learn those parameters W and b from the labeled data with “squares” and “circles” in our example, to later label the new point “X”.

위에 신경의 함수를 정의한 것을 활용하여, 신경이 어떻게 "사각형"과 "원"으로 레이블 된 데이터에서 생성된 파라메터 W와 b값으로 새로운 점인 "X"를 학습하는 과정을 예제로 통해 알아 볼 것이다.

A first approach could be similar to what we did with the linear regression, this is, feed the neuron with the known labeled data, and compare the obtained result with the real one. Then, when iterating, weights in W and b are adjusted to minimize the error, as shown again in chapter 2 for the linear regression line.

 시작은 선형회귀 때 접근했던 방법과 유사하다.  주어진 레이블 된 데이터를 뉴런에게 제공한 결과에 실제 값과 비교하는 것이다.  그 다음 이 과정을 반복하면서, 2장에서 회귀분석 선의 오류율을 최소화 한 것처럼 가중치인 W와 b 값을 조절한다.



-----------------------------------------------------------------





Once we have the W and b parameters we can compute the weighted sum, and now we need the function to turn the result stored in z into a ‘0’ or ‘1’. There are several activation functions available, and for this example we can use a popular one called sigmoid [33], returning a real value between 0 and 1


일단 W 와 b 파라메타를 구해서 가중치 합(역주: W*x + b 값)을 계산하고 나면 z 에 저장된 결과를 ‘0’ 또는 ‘1’ 로 바꿀 함수가 필요합니다. 몇 종류의 활성화 함수가 있지만 여기에서는 0 과 1 사이의 실수를 리턴하는 유명한 시그모이드(sigmoid) 함수를 사용하겠습니다.

:

image046

Looking at the formula we see that it will tend to return values close to 0 or 1. If input z is big enough and positive, “e” powered to minus z is zero and then y is 1. If the input z is big enough and negative, “e” powered to a large positive number becomes also a large positive number, so the denominator becomes large and the final y becomes 0. If we plot the function it would look like this:

이 공식을 보면 0 과 1 에 가까운 값으로 수렴하는 경향이 있는 것을 알 수 있습니다. 만약 z 가 충분히 큰 양수이면 e^{-z} 는 0 에 가까워지고 결국 y 는 1 이 됩니다. 만약 z 가 충분히 큰 음수면 e^{-z} 는 매우 큰 양수가 되고 분모가 커지므로 y 는 0 이 됩니다. 이 함수를 그림으로 나타내면 아래와 같습니다.


image045

Since here we have presented how to define a neuron, but a neural network is actually a composition of neurons connected among them in a different ways and using different activation functions. Given the scope of this book, I’ll not enter into all the extension of the neural networks universe, but I assure you that it is really exciting.

여기서 우리는 하나의 뉴런에 대해서 정의를 했지만 뉴럴 네트워크는 사실상 여러가지 방식으로 연결되어 있고 각기 다른 활성화 함수들을 사용하는 뉴런들을 합쳐 놓은 것입니다. 이 책의 구성상 뉴럴 네트워크 세계의 모든 면면을 살펴 볼 수는 없을 것 같습니다만 진짜 흥미로운 분야입니다.

Just to mention that there is a specific case of neural networks (in which Chapter 5 is based on) where the neurons are organized in layers, in a way where the inferior layer (input layer) receives the inputs, and the top layer (output layer) produces the response values. The neural network can have several intermediate layers, called hidden layers. A visual way to represent this is:

뉴럴 네트워크의 전형적인 구조는(5장에서 사용하게 됨) 입력을 받는 하위 레이어(입력 레이어), 결과 값을 내는 상위 레이어(출력 레이어)처럼 여러개의 레이어로 뉴런을 구성하는 것입니다. 뉴럴 네트워크는 히든(hidden) 레이어라 불리는 여러개의 중간 레이어를 가질 수 있습니다. 그림으로 보면 아래와 같습니다.

image049

In these networks, the neurons of a layer communicate to the neurons of the previous layer to receive information, and then communicate their results to the neurons of the next layer.

이런 네트워크에서는 한 레이어의 뉴런들은 정보를 받기 위해 이전 레이어의 뉴런과 커뮤니케이션 하고 결과를 내보내기 위해 다음 레이어의 뉴런과 커뮤니케이션 합니다.

As previously said, there are more activation functions apart of the Sigmoid, each one with different properties. For example, when we want to classify data into more than two classes at the output layer, we can use the Softmax[34] activation function, a generalization of the sigmoid function. Softmax allows obtaining the probability of each class, so their sum is 1 and the most probable result is the one with higher probability.

앞서 이야기한 것 처럼 시그모이드 외에도 각기 다른 특성을 가진 여러가지 활성화 함수가 있습니다. 예를 들면 출력 레이어에서 두가지 이상의 클래스로 데이터를 분류하고 싶을 때에는 시그모이드 함수의 일반화된 형태인 소프트맥스 활성화 함수를 사용할 수 있습니다. 소프트맥스 함수는 각 클래스(역주: 카테고리)에 대한 확률을 만들어 냅니다. 각 클래스의 확률의 합은 1 이고 가장 높은 값을 가진 클래스가 결과 값이 될 가능성이 가장 높습니다.

A easy example to start: Softmax

간단한 예제: 소프트맥스

Remember that the problem to solve is that, given an input image, we get the probability that it belongs to a certain digit. For example, our model could predict a “9” in an image with an 80% certainty, but give a 5% of chances to be an “8” (due to a dubious lower trace), and also give certain low probabilities to be any other number. There is some uncertainty on recognizing hand-written numbers, and we can’t recognize the digits with a 100% of confidence. In this case, a probability distribution gives us a better idea of how much confidence we have in our prediction.

여기서 풀려는 문제는 입력 이미지가 주어졌을 때 0~9 까지 각 숫자와 얼마나 비슷한지에 대한 확률을 얻으려는 것임을 기억해 두세요. 예를 들면 우리 모델이 어떤 이미지에 대해 “9” 일 확률이 80% 이고 “8” 일 확률이 5%(9의 애매한 꼬리 부분으로 인해) 이며 다른 숫자에 대해서는 낮은 확률을 예측할 수 있습니다. 손글씨 숫자를 인식하는 것엔 어느정도 불확실한 부분이 있고 100% 확신을 가지고 하긴 어렵습니다. 이런 경우에 확률 분포는 예측에 대해 얼마나 신뢰할 수 있는지에 대해 좋은 정보를 제공합니다.

So, we have an output vector with the probability distribution for the different output labels, mutully exclusive. This is, a vector with 10 probability values, each one corresponding to each digit from 0 to 9, and all probabilities summing 1.

그래서 우리는 상호 배타적인 레이블에 대한 결과로 확률 분포를 담은 출력 벡터를 가집니다. 10개의 확률 값을 가진 이 벡터는 각각 0 에서 9 까지의 숫자에 대응되는 것이고 확률의 전체 합은 1 입니다.

As previously said, we get to this by using an output layer with the softmax activation function. The output of a neuron with a softmax function depends on the output of the other neurons of its layer, as all of their outputs must sum 1.

앞서 언급한 것 처럼 이 벡터는 출력 레이어를 소프트맥스 활성화 함수로 구성하여 얻어집니다. 소프트맥스 함수를 사용한 뉴런의 출력 값은 그 레이어의 다른 뉴런의 출력 값에 영향을 받게 되고 그들의 출력 값 합은 1 이 되어야 합니다.

The softmax function has two main steps: first, the “evidences” for an image belonging to a certain label are computed, and later the evidences are converted into probabilities for each possible label.

소프트맥스 함수는 두개의 주요 단계가 있습니다. 첫번째는 이미지가 어떤 레이블에 속하는 지 근거(evidence)들을 계산하는 것이며 두번째는 근거들을 각 레이블에 대한 확률로 변환하는 것입니다.





----------------------------------------------------------------------------------------------------




Evidence of belonging
클래스 소속 근거

Measuring the evidence of a certain image to belong to a specific class/label, a usual approximation is to compute the weighted sum of pixel intensities. That weight is negative when a pixel with high intensity happens to not to be in a given class, and positive if the pixel is frequent in that class.

이미지가 어떤 클래스 혹은 레이블에 속하는 지의 근거를 측정하려면 보통 사용하는 방법은 픽셀의 진한 정도에 대한 가중치 합을 계산하는 것입니다. 어떤 클래스 픽셀에는 없는 진한 픽셀이 이미지에 있다면 가중치는 음의 값이 되고 클래스의 진한 픽셀이 이미지와 자주 겹친다면 가중치는 양의 값이 됩니다.

Let’s show a graphical example: suppose a learned model for the digit “0” (we will see how this is learned later). At this time, we define a model as “something” that contains information to know whether a number belongs to a specific class. In this case, we chose a model like the one below, where the red (or bright gray for the b/n edition) represents negative examples (this is, reduce the support for those pixels present in “0”), while the blue (the darker gray for b/n edition) represents the positive examples. Look at it:

그림으로 숫자 ‘0’ 을 학습한 모델의 예를 살펴 보겠습니다.(나중에 어떻게 학습되는지 살펴보겠습니다) 여기에서는 이미지가 어떤 클래스에 속할 지에 대한 정보를 가진 어떤 것이 모델이라고 정의합니다. 이 경우에는 아래와 같은 모델이 선택되었습니다. 붉은 색은 잘 맞지 않는 부분을 나타내고(즉 ‘0’ 에는 잘 나타나지 않는 픽셀임) 푸른 색은 잘 들어 맞는 부분임을 나타냅니다.

image050

Think in a white paper sheet of 28×28 pixels and draw a “0” on it. Generally our zero would be drawn in the blue zone (remember that we left some space around the 20×20 drawing zone, centering it later).

28×28 픽셀을 가진 흰 종이에 “0” 을 그려 본다고 생각해 봅시다. 일반적으로 ‘0’ 은 푸른 색 부분에 그려지게 될 것입니다.(20×20 그림 영역 주위에 약간의 여유를 두고 가운데를 맞춘 것임을 기억하세요)

It is clear that in those cases where our drawing crosses the red zone, it is most probably that we are not drawing a zero. So, using a metric based in rewarding those pixels, stepping on the blue region and penalizing those stepping the red zone seems reasonable.

붉은 영역을 가로지르는 이미지는 숫자 0 이 아니라고 생각하는 것이 자연스럽습니다. 그래서 푸른 영역에 위치한 픽셀들은 가산점을 주고 붉은 영역에 위치한 픽셀들은 페널티를 주는 측정 방법이 타당성이 있습니다.

Now think of a “3”: it is clear that the red zone of our model for “0” will penalize the probabilities of it being a “0”. But if the reference model is the following one, generally the pixels forming the “3” will follow the blue region; also the drawing of a “0” would step into a red zone.

이제 숫자 ‘3’ 을 생각해 봅시다. 숫자 ‘0’ 을 위한 모델에서는 붉은 영역이 ‘0’ 이 될 가능성을 낮추는 역할을 합니다. 하지만 아래와 같은 모델에서는 ‘3’ 을 구성하는 픽셀은 푸른 영역이 되고 ‘0’ 을 그리면 붉은 색위에 놓일 것 입니다.

image052

I hope that the reader, seeing those two examples, understands how the explained approximation allows us to estimate which number represent those drawings.
이 두 예제를 보면서 설명한 방법으로 어떻게 이미지의 숫자를 추정할 수 있는지 이해하는데 도움이 되었길 바랍니다.

The following figure shows an example of the ten different labels/classes learned from the MNIST data-set (extracted from the examples of Tensorflow[35]). Remember that red (bright gray) represents negative weights and blue (dark gray) represents the positive ones.
아래 그림은 MNIST 데이터셋에서 학습된 10개의 레이블(클래스)입니다.(텐서플로우 예제에서 추출함) 붉은 색은 음의 가중치를 나타내고 푸른색은 양의 가중치를 나타냅니다.
 

image054

In a more formal way, we can say that the evidence for a class i given an input x is expressed as:
좀 더 형식적으로 쓰면 입력 x 가 주어졌을 때 클래스 i 에 대한 근거는 아래와 같이 쓸 수 있습니다.

image056

Where i indicates the class (in our situation, between 0 and 9), and j is an index to sum the indexes of our input image. Finally Wi represents the aforementioned weights.
i 는 클래스(우리 예제에서는 0~9까지 숫자)를 나타내며 j 는 입력 이미지 중 하나를 나타냅니다. 결국 W_i 는 앞서 언급한 가중치를 나타냅니다.

Remember that, in general, the models also include an extra parameter representing the bias, adding some base uncertainty. In our situation, the formula would end like this:
일반적으로 모델은 약간의 불확실성을 더해 편향성(bias)을 표현하는 추가적인 파라메타를 가집니다. 그러므로 공식은 아래와 같이 됩니다.


image058

For each i (between 0 and 9) we have a matrix Wi of 784 elements (28×28), where each element j is multiplied by the corresponding component j of the input image, with 784 components, then added bi. A graphical view of the matrix calculus and indexes is this:
각 i (0에서 9까지 숫자)에 대해 784개 엘리먼트(28×28)를 가지는 행렬 W_i 를 얻게 됩니다. W 의 각 엘리먼트 j 는 입력 이미지의 784개의 콤포넌트(역주: 픽셀) j 에 곱해지고 b_i 가 더해 집니다. 그림으로 행렬 연산과 인덱스를 보면 아래와 같습니다.


image061

Probability of belonging
클래스 소속 확률

We commented that the second step consisted on computing probabilities. Specifically we turn the sum of evidences into predicted probabilities, indicated as y, using the softmax function:
확률을 계산하는 두번째 단계를 설명하겠습니다. 특별히 소프트맥스 함수를 사용하여 근거들의 합을 예측 확률 y 로 산출할 것입니다.

image062

Remember that the output vector must be a probability function with sum equals 1. To normalize each component, the softmax function uses the exponential value of each of its inputs and then normalizes them as follows:
출력 벡터는 합이 1 인 확률 함수가 되어야 합니다. 각 콤포넌트(역주: 벡터의 원소)를 정규화하기 위해 소프트맥스 함수는 입력 값을 모두 지수 값으로 바꾸어 아래와 같이 정규화합니다.

image064

The obtained effect when using exponentials is a multiplication effect in weights. Also, when the evidence for a class is small, this class support is reduced by a fraction of its previous weight. Furthermore, softmax normalizes the weights making them to sum 1, creating a probability distribution.
지수 함수를 사용하면 가중치를 크게하는 효과를 얻을 수 있습니다. 또한 한 클래스의 근거가 작을 때 이 클래스의 확률도 더 낮아지게 됩니다. 뿐만아니라 소프트맥스는 가중치의 합이 1 이 되도록 정규화하여 확률 분포를 만들어 줍니다.


The interesting fact of such function is that a good prediction will have one output with a value near 1, while all the other outputs will be near zero; and in a weak prediction, some labels may show similar support.
이런 함수의 좋은 점은 예측이 잘 되면 1 에 가까운 값이 하나가 있게 되고 다른 출력 값은 0 에 가깝게 되는 것입니다. 하지만 예측 값이 뚜렷하지 않을 때는 여러 레이블이 비슷한 확률을 가지게 될 수 있습니다.
